{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3abac9e5-1986-43f2-8cd7-feb7db263520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Section 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4fa5a4e-3347-4236-b089-21007241a9d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, current_timestamp, to_json, from_json, col, udf, explode, lit\n",
    "from pyspark.sql.functions import col, concat_ws\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, IntegerType, TimestampType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Configuration\n",
    "CATALOG_NAME = \"tulip_sandbox\"  # Change this to your catalog name\n",
    "SCHEMA_NAME = \"tulip_tables\"         # Change this to your schema name\n",
    "METADATA_SCHEMA_NAME = \"tulip_tables_metadata\" # Separate schema for metadata\n",
    "BATCH_SIZE = 100\n",
    "MAX_RETRIES = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2afd956c-5b69-47bc-bcdb-63dee1e44aa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Section 2: API Configuration and Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0685749-17b0-4fdf-8a90-851dbdc9d056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TulipConfig:\n",
    "    auth_header: str\n",
    "    base_url: str\n",
    "    \n",
    "    @classmethod\n",
    "    def from_env(cls) -> 'TulipConfig':\n",
    "        auth_header = \"Basic \" + os.getenv('AUTH')\n",
    "        base_url = 'https://william.tulip.co/api/v3'\n",
    "        return cls(auth_header=auth_header, base_url=base_url)\n",
    "\n",
    "class TulipAPI:\n",
    "    def __init__(self, config: TulipConfig):\n",
    "        self.config = config\n",
    "        self.headers = {\n",
    "            'Authorization': config.auth_header,\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "    \n",
    "    def get_tables(self) -> List[Dict[str, Any]]:\n",
    "        response = requests.get(f\"{self.config.base_url}/tables\", headers=self.headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def get_table_records(self, table_id: str, offset: int = 0) -> List[Dict[str, Any]]:\n",
    "        params = {\n",
    "            \"limit\": str(BATCH_SIZE),\n",
    "            \"offset\": str(offset),\n",
    "            \"includeTotalCount\": \"true\",\n",
    "            \"filterAggregator\": \"all\"\n",
    "        }\n",
    "        response = requests.get(\n",
    "            f\"{self.config.base_url}/tables/{table_id}/records\",\n",
    "            headers=self.headers,\n",
    "            params=params\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3f3eb0a-cd28-461a-bf83-76a4dadd148c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Section 3: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7b66bd7-1d4f-4e51-aa58-f3b8f9ffd18f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def sanitize_table_name(name: str) -> str:\n",
    "    return re.sub(r'[^a-zA-Z0-9_]', '_', name).lower()\n",
    "\n",
    "def map_tulip_type_to_spark_type(tulip_type: str) -> Any:\n",
    "    \"\"\"Map Tulip data types to Spark data types objects\"\"\"\n",
    "    type_mapping = {\n",
    "        'string': StringType(),\n",
    "        'float': DoubleType(),\n",
    "        'boolean': BooleanType(),\n",
    "        'integer': IntegerType(),\n",
    "        'timestamp': TimestampType(),\n",
    "        'interval': StringType(),\n",
    "        'color': StringType(),\n",
    "        'imageUrl': StringType(),\n",
    "        'tableLink': StringType()\n",
    "    }\n",
    "    return type_mapping.get(tulip_type, StringType())  # Default to StringType for unknown types\n",
    "\n",
    "def validate_schema_compatibility(spark: SparkSession, df, table_name: str) -> bool:\n",
    "    \"\"\"Validate if DataFrame schema is compatible with existing table\"\"\"\n",
    "    try:\n",
    "        # Get the table schema\n",
    "        table_df = spark.table(table_name)\n",
    "        table_schema = table_df.schema\n",
    "        df_schema = df.schema\n",
    "        \n",
    "        # Check for columns that exist in both with different types\n",
    "        for table_field in table_schema.fields:\n",
    "            for df_field in df_schema.fields:\n",
    "                if table_field.name == df_field.name and table_field.dataType != df_field.dataType:\n",
    "                    print(f\"Schema conflict: Column '{table_field.name}' has different types - \"\n",
    "                          f\"Table: {table_field.dataType}, DataFrame: {df_field.dataType}\")\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating schema: {str(e)}\")\n",
    "        return True  # Assume compatible if we can't check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d2a826-0c47-4c2a-b04d-dc24f23a8500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Section 4: Unity Catalog Table Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c48932-74dd-4026-872a-b6afaff88e86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_metadata_tables(spark: SparkSession):\n",
    "    # Create metadata schema if it doesn't exist\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{METADATA_SCHEMA_NAME}\")\n",
    "    \n",
    "    # Create tables table\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {CATALOG_NAME}.{METADATA_SCHEMA_NAME}.tables (\n",
    "            id STRING,\n",
    "            name STRING,\n",
    "            description STRING,\n",
    "            workspace_id STRING,\n",
    "            created_at TIMESTAMP,\n",
    "            updated_at TIMESTAMP\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create columns table\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {CATALOG_NAME}.{METADATA_SCHEMA_NAME}.columns (\n",
    "            table_id STRING,\n",
    "            column_name STRING,\n",
    "            column_label STRING,\n",
    "            data_type STRING,\n",
    "            is_unique BOOLEAN\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "\n",
    "def update_metadata_tables(spark: SparkSession, table_metadata: List[Dict[str, Any]]):\n",
    "    # Create tables DataFrame\n",
    "    tables_data = [\n",
    "        (table[\"id\"], sanitize_table_name(table[\"label\"]), table[\"description\"], \n",
    "         table[\"workspaceID\"], table[\"createdAt\"], table[\"updatedAt\"])\n",
    "        for table in table_metadata\n",
    "    ]\n",
    "    tables_df = spark.createDataFrame(\n",
    "        tables_data,\n",
    "        [\"id\", \"name\", \"description\", \"workspace_id\", \"created_at\", \"updated_at\"]\n",
    "    )\n",
    "    \n",
    "    # Create columns DataFrame\n",
    "    columns_data = []\n",
    "    for table in table_metadata:\n",
    "        for column in table[\"columns\"]:\n",
    "            if not column[\"hidden\"]:\n",
    "                columns_data.append((\n",
    "                    table[\"id\"],\n",
    "                    column[\"name\"],\n",
    "                    column[\"label\"],\n",
    "                    map_tulip_type_to_spark_type(column[\"dataType\"][\"type\"]).typeName(),\n",
    "                    column[\"unique\"]\n",
    "                ))\n",
    "    columns_df = spark.createDataFrame(\n",
    "        columns_data,\n",
    "        [\"table_id\", \"column_name\", \"column_label\", \"data_type\", \"is_unique\"]\n",
    "    )\n",
    "    \n",
    "    # Update tables\n",
    "    tables_df.createOrReplaceTempView(\"tables_df_view\")\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {CATALOG_NAME}.{METADATA_SCHEMA_NAME}.tables AS target\n",
    "        USING tables_df_view AS source\n",
    "        ON target.id = source.id\n",
    "        WHEN MATCHED THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "    \n",
    "    # Update columns\n",
    "    columns_df.createOrReplaceTempView(\"columns_df_view\")\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {CATALOG_NAME}.{METADATA_SCHEMA_NAME}.columns AS target\n",
    "        USING columns_df_view AS source\n",
    "        ON target.table_id = source.table_id AND target.column_name = source.column_name\n",
    "        WHEN MATCHED THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faf19c3b-1f68-4e90-9eef-028b0d7dae56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Section 5: Data Table Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "615565c0-0e80-45f4-9d9c-fd648b4664a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_table_schema(table_metadata: Dict[str, Any]) -> StructType:\n",
    "    \"\"\"Build Spark schema from table metadata with robust type handling\"\"\"\n",
    "    fields = []\n",
    "    for col in table_metadata['columns']:\n",
    "        if not col['hidden']:\n",
    "            col_type = map_tulip_type_to_spark_type(col['dataType']['type'])\n",
    "            \n",
    "            # Provide explicit nullability information\n",
    "            nullable = True\n",
    "            if 'required' in col and col['required'] is True:\n",
    "                nullable = False\n",
    "                \n",
    "            fields.append(StructField(col['name'], col_type, nullable))\n",
    "            \n",
    "            # Debug logging for column types\n",
    "            print(f\"Column '{col['name']}': Tulip type '{col['dataType']['type']}' mapped to Spark type '{col_type}'\")\n",
    "            \n",
    "    return StructType(fields)\n",
    "\n",
    "def create_data_table(spark: SparkSession, table_metadata: Dict[str, Any]):\n",
    "    \"\"\"Create or update Delta table based on the Tulip table metadata\"\"\"\n",
    "    table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{sanitize_table_name(table_metadata['label'])}\"\n",
    "    \n",
    "    # Use explicit schema instead of string-based schema\n",
    "    schema = build_table_schema(table_metadata)\n",
    "    \n",
    "    # Check if table already exists\n",
    "    try:\n",
    "        existing_table = spark.table(table_name)\n",
    "        print(f\"Table {table_name} already exists, checking schema compatibility\")\n",
    "        \n",
    "        # Create a sample DataFrame to check schema compatibility\n",
    "        empty_df = spark.createDataFrame([], schema)\n",
    "        \n",
    "        # Check if schema is compatible\n",
    "        schema_compatible = validate_schema_compatibility(spark, empty_df, table_name)\n",
    "        if not schema_compatible:\n",
    "            print(f\"Schema has changed for table {table_name}. Schema update may be required.\")\n",
    "            # Note: In a production setting, you would implement schema evolution strategies here\n",
    "            # For this script, we'll continue with the existing table\n",
    "    except:\n",
    "        print(f\"Creating new table {table_name}\")\n",
    "        # Create empty DataFrame with schema\n",
    "        empty_df = spark.createDataFrame([], schema)\n",
    "        # Create table\n",
    "        empty_df.write.format(\"delta\").mode(\"ignore\").saveAsTable(table_name)\n",
    "    \n",
    "    return table_name\n",
    "\n",
    "def process_record_batch(spark: SparkSession, table_name: str, schema: StructType, records: List[Dict[str, Any]]):\n",
    "    \"\"\"Process a batch of records with explicit schema and type handling\"\"\"\n",
    "    if not records:\n",
    "        return 0\n",
    "        \n",
    "    # Convert records to match schema\n",
    "    processed_records = []\n",
    "    for record in records:\n",
    "        row = {}\n",
    "        for field in schema.fields:\n",
    "            if field.name in record:\n",
    "                value = record[field.name]\n",
    "                # Handle null values\n",
    "                if value is None:\n",
    "                    row[field.name] = None\n",
    "                # Handle type conversions safely based on field type\n",
    "                elif isinstance(field.dataType, DoubleType):\n",
    "                    try:\n",
    "                        row[field.name] = float(value) if value is not None else None\n",
    "                    except (ValueError, TypeError):\n",
    "                        row[field.name] = None\n",
    "                        print(f\"Warning: Could not convert value '{value}' to float for field '{field.name}'\")\n",
    "                elif isinstance(field.dataType, IntegerType):\n",
    "                    try:\n",
    "                        row[field.name] = int(value) if value is not None else None\n",
    "                    except (ValueError, TypeError):\n",
    "                        row[field.name] = None\n",
    "                        print(f\"Warning: Could not convert value '{value}' to integer for field '{field.name}'\")\n",
    "                elif isinstance(field.dataType, BooleanType):\n",
    "                    try:\n",
    "                        if isinstance(value, bool):\n",
    "                            row[field.name] = value\n",
    "                        elif isinstance(value, str):\n",
    "                            row[field.name] = value.lower() in ('true', 't', 'yes', 'y', '1')\n",
    "                        elif isinstance(value, (int, float)):\n",
    "                            row[field.name] = bool(value)\n",
    "                        else:\n",
    "                            row[field.name] = None\n",
    "                    except (ValueError, TypeError):\n",
    "                        row[field.name] = None\n",
    "                        print(f\"Warning: Could not convert value '{value}' to boolean for field '{field.name}'\")\n",
    "                elif isinstance(field.dataType, TimestampType):\n",
    "                    try:\n",
    "                        row[field.name] = value\n",
    "                    except (ValueError, TypeError):\n",
    "                        row[field.name] = None\n",
    "                        print(f\"Warning: Could not convert value '{value}' to timestamp for field '{field.name}'\")\n",
    "                else:\n",
    "                    # Default to string conversion for other types\n",
    "                    try:\n",
    "                        row[field.name] = str(value) if value is not None else None\n",
    "                    except Exception:\n",
    "                        row[field.name] = None\n",
    "                        print(f\"Warning: Could not convert value to string for field '{field.name}'\")\n",
    "            else:\n",
    "                row[field.name] = None\n",
    "        processed_records.append(row)\n",
    "    \n",
    "    # Create DataFrame with explicit schema\n",
    "    df = spark.createDataFrame(processed_records, schema)\n",
    "    \n",
    "    # Validate schema compatibility\n",
    "    schema_compatible = validate_schema_compatibility(spark, df, table_name)\n",
    "    if not schema_compatible:\n",
    "        print(f\"Schema incompatibility detected. Trying to harmonize schema...\")\n",
    "        # Get table schema as a reference\n",
    "        try:\n",
    "            table_df = spark.table(table_name)\n",
    "            for field in table_df.schema.fields:\n",
    "                if field.name in df.columns:\n",
    "                    df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "        except Exception as e:\n",
    "            print(f\"Error harmonizing schema: {str(e)}\")\n",
    "    \n",
    "    # Write to table\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "    \n",
    "    return len(processed_records)\n",
    "\n",
    "def fetch_table_data(api: TulipAPI, spark: SparkSession, table_metadata: Dict[str, Any], table_name: str):\n",
    "    \"\"\"Fetch and process table data in batches with robust error handling\"\"\"\n",
    "    table_id = table_metadata[\"id\"]\n",
    "    schema = build_table_schema(table_metadata)\n",
    "    \n",
    "    total_records = 0\n",
    "    offset = 0\n",
    "    batch_counter = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            print(f\"Fetching batch {batch_counter} (offset {offset}) for table {table_name}\")\n",
    "            data = api.get_table_records(table_id, offset)\n",
    "            \n",
    "            if not data:\n",
    "                print(f\"No more data for table {table_name}\")\n",
    "                break\n",
    "                \n",
    "            # Process batch with explicit schema\n",
    "            records_processed = process_record_batch(spark, table_name, schema, data)\n",
    "            \n",
    "            if records_processed == 0:\n",
    "                break\n",
    "                \n",
    "            total_records += records_processed\n",
    "            offset += BATCH_SIZE\n",
    "            batch_counter += 1\n",
    "            \n",
    "            print(f\"Processed {records_processed} records (total: {total_records})\")\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API error fetching data for table {table_name}: {e}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch for table {table_name}: {e}\")\n",
    "            break\n",
    "    \n",
    "    return total_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06bd02a4-3bde-4444-acfc-44e6159295a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Section 6: Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e21a4d-1936-4a16-b5c4-732df64df2c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize API client\n",
    "    config = TulipConfig.from_env()\n",
    "    api = TulipAPI(config)\n",
    "    \n",
    "    try:\n",
    "        # Get table metadata\n",
    "        table_metadata = api.get_tables()\n",
    "        \n",
    "        # Create and update metadata tables\n",
    "        create_metadata_tables(spark)\n",
    "        update_metadata_tables(spark, table_metadata)\n",
    "        \n",
    "        # Process each table\n",
    "        for table in table_metadata:\n",
    "            try:\n",
    "                # Create table in Unity Catalog\n",
    "                table_name = create_data_table(spark, table)\n",
    "                \n",
    "                # Fetch and load data with robust schema handling\n",
    "                total_records = fetch_table_data(api, spark, table, table_name)\n",
    "                \n",
    "                print(f\"Successfully processed table: {table['label']} with {total_records} records\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing table {table['label']}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(\"All tables processed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main process: {e}\")\n",
    "        raise\n",
    "\n",
    "# Run the main process\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "rest_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
