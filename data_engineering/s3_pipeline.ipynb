{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad43364-2be3-40f6-bd52-24198b88950a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Section 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197b5800-7688-4d7d-9e95-1dd3586aa658",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Dict, Any\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import boto3\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import col, lit, current_timestamp, input_file_name, regexp_extract, concat\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType, ArrayType, MapType, DoubleType\n",
    "\n",
    "# Configuration constants\n",
    "CATALOG_NAME = \"tulip_sandbox\"\n",
    "SCHEMA_NAME = \"sitewise\"\n",
    "S3_BUCKET = os.getenv('BUCKET_NAME', 'hannover-messe-tulip')\n",
    "BASE_PREFIX = os.getenv('BASE_PREFIX', 'iot-sitewise/')\n",
    "METADATA_TABLE = f\"{CATALOG_NAME}.{SCHEMA_NAME}.file_metadata\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\")\n",
    "\n",
    "# Known subdirectories and their file types\n",
    "KNOWN_DIRECTORIES = {\n",
    "    \"agg\": \"avro\",\n",
    "    \"index\": \"avro\",\n",
    "    \"raw\": \"avro\"\n",
    "}\n",
    "\n",
    "# File path regex patterns\n",
    "AGG_RAW_PATTERN = r\"startYear=(\\d{4})/startMonth=(\\d{1,2})/startDay=(\\d{1,2})/seriesBucket=([^/]+)/(?:agg|raw)_([^_]+)_(\\d+)_GOOD\\.avro$\"\n",
    "INDEX_PATTERN = r\"series=([^/]+)/startYear=(\\d{4})/startMonth=(\\d{1,2})/startDay=(\\d{1,2})/index_\\1_(\\d+)_GOOD$\"\n",
    "\n",
    "def get_aws_credentials():\n",
    "    \"\"\"Get AWS credentials from environment variables\"\"\"\n",
    "    access_key = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "    secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "    \n",
    "    if not access_key or not secret_key:\n",
    "        raise ValueError(\"AWS credentials not found in environment variables\")\n",
    "    \n",
    "    return access_key, secret_key\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21fb397e-6058-4f09-9d7e-2807d6f66d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Section 2: Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ef7d978-c9e1-4f40-b0c5-c59a8a28dcb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class S3Config:\n",
    "    \"\"\"Configuration for S3 access\"\"\"\n",
    "    bucket: str\n",
    "    aws_access_key_id: str\n",
    "    aws_secret_access_key: str\n",
    "    region: str = os.getenv('AWS_REGION', 'us-east-1')\n",
    "    base_prefix: str = BASE_PREFIX\n",
    "\n",
    "    @classmethod\n",
    "    def from_env(cls) -> 'S3Config':\n",
    "        \"\"\"Create S3Config from environment variables\"\"\"\n",
    "        access_key, secret_key = get_aws_credentials()\n",
    "        return cls(\n",
    "            bucket=S3_BUCKET,\n",
    "            aws_access_key_id=access_key,\n",
    "            aws_secret_access_key=secret_key\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class FileMetadata:\n",
    "    \"\"\"Metadata about processed files\"\"\"\n",
    "    file_path: str\n",
    "    file_type: str  # 'avro' or 'jsonl'\n",
    "    directory: str\n",
    "    last_modified: datetime\n",
    "    size: int\n",
    "    table_name: str\n",
    "    processed_at: datetime = datetime.now()\n",
    "    record_count: Optional[int] = None\n",
    "    error_message: Optional[str] = None\n",
    "    year: Optional[int] = None\n",
    "    month: Optional[int] = None\n",
    "    day: Optional[int] = None\n",
    "    series_bucket: Optional[str] = None\n",
    "    series_id: Optional[str] = None\n",
    "    timestamp: Optional[int] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2274569c-cf3c-4a2e-979b-62dc9346f7b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Section 3: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff2ba62-b812-437c-9445-784ec2dd732f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def sanitize_table_name(name: str) -> str:\n",
    "    \"\"\"Convert directory name to valid table name\"\"\"\n",
    "    return name.lower().replace('-', '_').replace(' ', '_')\n",
    "\n",
    "def get_s3_client(config: S3Config) -> boto3.client:\n",
    "    \"\"\"Create S3 client with explicit credentials\"\"\"\n",
    "    return boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=config.aws_access_key_id,\n",
    "        aws_secret_access_key=config.aws_secret_access_key,\n",
    "        region_name=config.region\n",
    "    )\n",
    "\n",
    "def extract_partition_info(file_path: str, directory: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract partition information from file path based on directory type\"\"\"\n",
    "    if directory in [\"agg\", \"raw\"]:\n",
    "        match = re.search(AGG_RAW_PATTERN, file_path)\n",
    "        if match:\n",
    "            return {\n",
    "                \"year\": int(match.group(1)),\n",
    "                \"month\": int(match.group(2)),\n",
    "                \"day\": int(match.group(3)),\n",
    "                \"series_bucket\": match.group(4),\n",
    "                \"asset_id\": match.group(5),\n",
    "                \"timestamp\": int(match.group(6))\n",
    "            }\n",
    "    elif directory == \"index\":\n",
    "        match = re.search(INDEX_PATTERN, file_path)\n",
    "        if match:\n",
    "            return {\n",
    "                \"series_id\": match.group(1),\n",
    "                \"year\": int(match.group(2)),\n",
    "                \"month\": int(match.group(3)),\n",
    "                \"day\": int(match.group(4)),\n",
    "                \"timestamp\": int(match.group(5))\n",
    "            }\n",
    "    return {}\n",
    "\n",
    "def list_s3_files(client: boto3.client, config: S3Config) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"List files in S3 bucket organized by subdirectory\"\"\"\n",
    "    files_by_directory = {dir_name: [] for dir_name in KNOWN_DIRECTORIES.keys()}\n",
    "    \n",
    "    for directory, file_type in KNOWN_DIRECTORIES.items():\n",
    "        prefix = f\"{config.base_prefix}{directory}/\"\n",
    "        paginator = client.get_paginator('list_objects_v2')\n",
    "        \n",
    "        for page in paginator.paginate(Bucket=config.bucket, Prefix=prefix):\n",
    "            if 'Contents' not in page:\n",
    "                continue\n",
    "                \n",
    "            for obj in page['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith('/'):  # Skip directories\n",
    "                    continue\n",
    "                    \n",
    "                # Only process files with matching extension\n",
    "                if (file_type == 'avro' and key.endswith('.avro')) or \\\n",
    "                   (file_type == 'jsonl' and key.endswith('.jsonl')):\n",
    "                    partition_info = extract_partition_info(key, directory)\n",
    "                    \n",
    "                    files_by_directory[directory].append({\n",
    "                        'key': key,\n",
    "                        'type': file_type,\n",
    "                        'directory': directory,\n",
    "                        'last_modified': obj['LastModified'],\n",
    "                        'size': obj['Size'],\n",
    "                        **partition_info\n",
    "                    })\n",
    "    \n",
    "    return files_by_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ece080a-f639-449a-8be6-0e8e05b81b15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Section 4: Unity Catalog Table Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb895e8-938c-4457-9a92-1789086ae308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_metadata_table():\n",
    "    \"\"\"Create metadata table in Unity Catalog if it doesn't exist\"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"file_path\", StringType(), False),\n",
    "        StructField(\"file_type\", StringType(), False),\n",
    "        StructField(\"directory\", StringType(), False),\n",
    "        StructField(\"last_modified\", TimestampType(), False),\n",
    "        StructField(\"size\", LongType(), False),\n",
    "        StructField(\"table_name\", StringType(), False),\n",
    "        StructField(\"processed_at\", TimestampType(), False),\n",
    "        StructField(\"record_count\", LongType(), True),\n",
    "        StructField(\"error_message\", StringType(), True),\n",
    "        StructField(\"year\", LongType(), True),\n",
    "        StructField(\"month\", LongType(), True),\n",
    "        StructField(\"day\", LongType(), True),\n",
    "        StructField(\"series_bucket\", StringType(), True),\n",
    "        StructField(\"series_id\", StringType(), True),\n",
    "        StructField(\"timestamp\", LongType(), True)\n",
    "    ])\n",
    "    \n",
    "    empty_df = spark.createDataFrame([], schema)\n",
    "    empty_df.write.format(\"delta\").mode(\"ignore\").saveAsTable(METADATA_TABLE)\n",
    "\n",
    "def update_metadata(metadata: List[FileMetadata]):\n",
    "    \"\"\"Update metadata table with processed file information\"\"\"\n",
    "    metadata_df = spark.createDataFrame([vars(m) for m in metadata])\n",
    "    metadata_df.write.format(\"delta\").mode(\"append\").saveAsTable(METADATA_TABLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c764c617-bc1b-47d9-9d8f-36948936453a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Section 5: Data Table Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec19227-d3f4-4376-89f9-042db38e72be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_processed_files() -> List[str]:\n",
    "    \"\"\"Get list of already processed files from metadata table\"\"\"\n",
    "    return [row.file_path for row in spark.table(METADATA_TABLE).select(\"file_path\").distinct().collect()]\n",
    "\n",
    "def infer_schema_with_fallback(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Infer schema with fallback to string type for problematic columns\"\"\"\n",
    "    # Get the current schema\n",
    "    current_schema = df.schema\n",
    "    \n",
    "    # Create a new schema with fallback types\n",
    "    new_fields = []\n",
    "    problematic_columns = []\n",
    "    \n",
    "    for field in current_schema.fields:\n",
    "        try:\n",
    "            # If the field type is NullType or can't be determined, default to StringType\n",
    "            if field.dataType.simpleString() == 'null' or field.dataType.simpleString() == 'void':\n",
    "                problematic_columns.append(f\"{field.name}: NullType/void type\")\n",
    "                new_fields.append(StructField(field.name, StringType(), field.nullable))\n",
    "            # For numeric types, default to DoubleType\n",
    "            elif field.dataType.simpleString() in ['int', 'bigint', 'smallint', 'tinyint', 'float', 'decimal']:\n",
    "                new_fields.append(StructField(field.name, DoubleType(), field.nullable))\n",
    "            # For timestamp types, ensure proper handling\n",
    "            elif field.dataType.simpleString() == 'timestamp':\n",
    "                new_fields.append(StructField(field.name, TimestampType(), field.nullable))\n",
    "            # For array types, default to ArrayType(StringType())\n",
    "            elif isinstance(field.dataType, ArrayType):\n",
    "                new_fields.append(StructField(field.name, ArrayType(StringType()), field.nullable))\n",
    "            # For map types, default to MapType(StringType(), StringType())\n",
    "            elif isinstance(field.dataType, MapType):\n",
    "                new_fields.append(StructField(field.name, MapType(StringType(), StringType()), field.nullable))\n",
    "            # For all other types, keep the original type\n",
    "            else:\n",
    "                new_fields.append(field)\n",
    "        except Exception as e:\n",
    "            problematic_columns.append(f\"{field.name}: {str(e)}\")\n",
    "            print(f\"Warning: Could not determine type for column {field.name}, defaulting to StringType: {str(e)}\")\n",
    "            new_fields.append(StructField(field.name, StringType(), field.nullable))\n",
    "    \n",
    "    if problematic_columns:\n",
    "        print(\"\\nType inference issues found in the following columns:\")\n",
    "        print(problematic_columns)\n",
    "        print(\"\\nThese columns will be defaulted to StringType.\\n\")\n",
    "    \n",
    "    # Create new schema\n",
    "    new_schema = StructType(new_fields)\n",
    "    \n",
    "    # Cast all columns to their new types\n",
    "    for field in new_schema.fields:\n",
    "        if field.name in df.columns:\n",
    "            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_table_for_directory(directory: str, file_type: str) -> str:\n",
    "    \"\"\"Create Delta table for a directory if it doesn't exist\"\"\"\n",
    "    table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{sanitize_table_name(directory)}\"\n",
    "    \n",
    "    try:\n",
    "        spark.table(table_name)\n",
    "        print(f\"Table {table_name} already exists\")\n",
    "    except:\n",
    "        print(f\"Creating new table {table_name}\")\n",
    "        \n",
    "        # Use the first file to infer schema\n",
    "        s3_client = get_s3_client(S3Config.from_env())\n",
    "        prefix = f\"{BASE_PREFIX}{directory}/\"\n",
    "        \n",
    "        response = s3_client.list_objects_v2(\n",
    "            Bucket=S3_BUCKET,\n",
    "            Prefix=prefix,\n",
    "            MaxKeys=1\n",
    "        )\n",
    "        \n",
    "        if 'Contents' not in response:\n",
    "            raise ValueError(f\"No files found in {directory}\")\n",
    "            \n",
    "        sample_key = response['Contents'][0]['Key']\n",
    "        print(f\"Using {sample_key} as sample file for schema inference\")\n",
    "        \n",
    "        # Read sample file with string type fallback\n",
    "        sample_path = f\"s3a://{S3_BUCKET}/{sample_key}\"\n",
    "        if file_type == \"ndjson\":\n",
    "            sample_df = spark.read \\\n",
    "                .format(\"json\") \\\n",
    "                .option(\"multiline\", \"false\") \\\n",
    "                .option(\"lineSep\", \"\\n\") \\\n",
    "                .option(\"primitivesAsString\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"false\") \\\n",
    "                .load(sample_path)\n",
    "        else:\n",
    "            sample_df = spark.read \\\n",
    "                .format(file_type) \\\n",
    "                .option(\"inferSchema\", \"false\") \\\n",
    "                .load(sample_path)\n",
    "        \n",
    "        # Apply robust schema inference with fallback\n",
    "        sample_df = infer_schema_with_fallback(sample_df)\n",
    "        \n",
    "        # Add partition columns with empty values if they don't exist\n",
    "        if directory in [\"agg\", \"raw\"]:\n",
    "            if \"year\" not in sample_df.columns:\n",
    "                sample_df = sample_df.withColumn(\"year\", lit(None).cast(LongType()))\n",
    "            if \"month\" not in sample_df.columns:\n",
    "                sample_df = sample_df.withColumn(\"month\", lit(None).cast(LongType()))\n",
    "            if \"day\" not in sample_df.columns:\n",
    "                sample_df = sample_df.withColumn(\"day\", lit(None).cast(LongType()))\n",
    "            if \"series_bucket\" not in sample_df.columns:\n",
    "                sample_df = sample_df.withColumn(\"series_bucket\", lit(None).cast(StringType()))\n",
    "            # Add unique identifier for deduplication\n",
    "            if \"unique_id\" not in sample_df.columns:\n",
    "                sample_df = sample_df.withColumn(\"unique_id\", \n",
    "                    concat(\n",
    "                        col(\"series_bucket\"),\n",
    "                        lit(\"_\"),\n",
    "                        col(\"asset_id\"),\n",
    "                        lit(\"_\"),\n",
    "                        col(\"timestamp\")\n",
    "                    )\n",
    "                )\n",
    "        elif directory == \"index\":\n",
    "            if \"series_id\" not in sample_df.columns:\n",
    "                sample_df = sample_df.withColumn(\"series_id\", lit(None).cast(StringType()))\n",
    "            if \"year\" not in sample_df.columns:\n",
    "                sample_df = sample_df.withColumn(\"year\", lit(None).cast(LongType()))\n",
    "            if \"month\" not in sample_df.columns:\n",
    "                sample_df = sample_df.withColumn(\"month\", lit(None).cast(LongType()))\n",
    "            if \"day\" not in sample_df.columns:\n",
    "                sample_df = sample_df.withColumn(\"day\", lit(None).cast(LongType()))\n",
    "            # Add unique identifier for deduplication\n",
    "            if \"unique_id\" not in sample_df.columns:\n",
    "                sample_df = sample_df.withColumn(\"unique_id\", \n",
    "                    concat(\n",
    "                        col(\"series_id\"),\n",
    "                        lit(\"_\"),\n",
    "                        col(\"timestamp\")\n",
    "                    )\n",
    "                )\n",
    "        else:  # asset_metadata\n",
    "            # For asset metadata, use asset_id as unique identifier\n",
    "            if \"unique_id\" not in sample_df.columns:\n",
    "                sample_df = sample_df.withColumn(\"unique_id\", col(\"asset_id\"))\n",
    "        \n",
    "        # Create table with appropriate partitioning and constraints\n",
    "        if directory in [\"agg\", \"raw\"]:\n",
    "            sample_df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"ignore\") \\\n",
    "                .partitionBy(\"year\", \"month\", \"day\", \"series_bucket\") \\\n",
    "                .saveAsTable(table_name)\n",
    "            \n",
    "            # Add unique constraint\n",
    "            spark.sql(f\"\"\"\n",
    "                ALTER TABLE {table_name} \n",
    "                ADD CONSTRAINT unique_id_constraint \n",
    "                UNIQUE (unique_id)\n",
    "            \"\"\")\n",
    "        elif directory == \"index\":\n",
    "            sample_df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"ignore\") \\\n",
    "                .partitionBy(\"series_id\", \"year\", \"month\", \"day\") \\\n",
    "                .saveAsTable(table_name)\n",
    "            \n",
    "            # Add unique constraint\n",
    "            spark.sql(f\"\"\"\n",
    "                ALTER TABLE {table_name} \n",
    "                ADD CONSTRAINT unique_id_constraint \n",
    "                UNIQUE (unique_id)\n",
    "            \"\"\")\n",
    "        else:  # asset_metadata\n",
    "            sample_df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"ignore\") \\\n",
    "                .saveAsTable(table_name)\n",
    "            \n",
    "            # Add unique constraint\n",
    "            spark.sql(f\"\"\"\n",
    "                ALTER TABLE {table_name} \n",
    "                ADD CONSTRAINT unique_id_constraint \n",
    "                UNIQUE (unique_id)\n",
    "            \"\"\")\n",
    "    \n",
    "    return table_name\n",
    "\n",
    "def process_directory_files(directory: str, files: List[Dict[str, Any]]) -> List[FileMetadata]:\n",
    "    \"\"\"Process all files in a directory\"\"\"\n",
    "    if not files:\n",
    "        print(f\"No files to process in directory {directory}\")\n",
    "        return []\n",
    "    \n",
    "    metadata_records = []\n",
    "    processed_files = set(get_processed_files())\n",
    "    file_type = KNOWN_DIRECTORIES[directory]\n",
    "    new_files = []  # Initialize new_files at the top level\n",
    "    \n",
    "    try:\n",
    "        table_name = create_table_for_directory(directory, file_type)\n",
    "        \n",
    "        # Process files not already processed\n",
    "        new_files = [f for f in files if f['key'] not in processed_files]\n",
    "        if not new_files:\n",
    "            print(f\"No new files to process in directory {directory}\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Found {len(new_files)} new files to process in {directory}\")\n",
    "        \n",
    "        if directory in [\"agg\", \"raw\"]:\n",
    "            # Group files by partition for agg and raw directories\n",
    "            files_by_partition = {}\n",
    "            for file_info in new_files:\n",
    "                partition_key = (\n",
    "                    file_info.get('year', ''),\n",
    "                    file_info.get('month', ''),\n",
    "                    file_info.get('day', ''),\n",
    "                    file_info.get('series_bucket', '')\n",
    "                )\n",
    "                if partition_key not in files_by_partition:\n",
    "                    files_by_partition[partition_key] = []\n",
    "                files_by_partition[partition_key].append(file_info)\n",
    "            \n",
    "            # Process each partition\n",
    "            for partition_key, partition_files in files_by_partition.items():\n",
    "                year, month, day, series_bucket = partition_key\n",
    "                print(f\"Processing partition: year={year}, month={month}, day={day}, series_bucket={series_bucket}\")\n",
    "                \n",
    "                file_paths = [f\"s3a://{S3_BUCKET}/{file_info['key']}\" for file_info in partition_files]\n",
    "                \n",
    "                try:\n",
    "                    # Read with more conservative schema inference\n",
    "                    df = spark.read \\\n",
    "                        .format(file_type) \\\n",
    "                        .option(\"inferSchema\", \"false\") \\\n",
    "                        .load(file_paths)\n",
    "                    \n",
    "                    # Apply robust schema inference with fallback\n",
    "                    df = infer_schema_with_fallback(df)\n",
    "                    \n",
    "                    # Add partition columns from file path information\n",
    "                    df = df.withColumn(\"year\", lit(year).cast(LongType()))\n",
    "                    df = df.withColumn(\"month\", lit(month).cast(LongType()))\n",
    "                    df = df.withColumn(\"day\", lit(day).cast(LongType()))\n",
    "                    df = df.withColumn(\"series_bucket\", lit(series_bucket).cast(StringType()))\n",
    "                    \n",
    "                    # Add unique identifier for deduplication\n",
    "                    df = df.withColumn(\"unique_id\", \n",
    "                        concat(\n",
    "                            col(\"series_bucket\"),\n",
    "                            lit(\"_\"),\n",
    "                            col(\"asset_id\"),\n",
    "                            lit(\"_\"),\n",
    "                            col(\"timestamp\")\n",
    "                        )\n",
    "                    )\n",
    "                    \n",
    "                    # Validate schema compatibility\n",
    "                    schema_compatible = validate_schema_compatibility(df, table_name)\n",
    "                    if not schema_compatible:\n",
    "                        print(f\"Schema incompatibility detected. Trying to harmonize schema...\")\n",
    "                        try:\n",
    "                            table_df = spark.table(table_name)\n",
    "                            for field in table_df.schema.fields:\n",
    "                                if field.name in df.columns:\n",
    "                                    df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error harmonizing schema: {str(e)}\")\n",
    "                    \n",
    "                    # Use merge to handle deduplication\n",
    "                    df.createOrReplaceTempView(\"source_df\")\n",
    "                    spark.sql(f\"\"\"\n",
    "                        MERGE INTO {table_name} AS target\n",
    "                        USING source_df AS source\n",
    "                        ON target.unique_id = source.unique_id\n",
    "                        WHEN MATCHED THEN UPDATE SET *\n",
    "                        WHEN NOT MATCHED THEN INSERT *\n",
    "                    \"\"\")\n",
    "                    \n",
    "                    record_count = df.count()\n",
    "                    for file_info in partition_files:\n",
    "                        metadata_records.append(FileMetadata(\n",
    "                            file_path=file_info['key'],\n",
    "                            file_type=file_type,\n",
    "                            directory=directory,\n",
    "                            last_modified=file_info['last_modified'],\n",
    "                            size=file_info['size'],\n",
    "                            table_name=table_name,\n",
    "                            record_count=record_count,\n",
    "                            year=file_info.get('year'),\n",
    "                            month=file_info.get('month'),\n",
    "                            day=file_info.get('day'),\n",
    "                            series_bucket=file_info.get('series_bucket')\n",
    "                        ))\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing partition: {str(e)}\")\n",
    "                    for file_info in partition_files:\n",
    "                        metadata_records.append(FileMetadata(\n",
    "                            file_path=file_info['key'],\n",
    "                            file_type=file_type,\n",
    "                            directory=directory,\n",
    "                            last_modified=file_info['last_modified'],\n",
    "                            size=file_info['size'],\n",
    "                            table_name=table_name,\n",
    "                            error_message=str(e),\n",
    "                            year=file_info.get('year'),\n",
    "                            month=file_info.get('month'),\n",
    "                            day=file_info.get('day'),\n",
    "                            series_bucket=file_info.get('series_bucket')\n",
    "                        ))\n",
    "        \n",
    "        elif directory == \"index\":\n",
    "            # Group files by partition for index directory\n",
    "            files_by_partition = {}\n",
    "            for file_info in new_files:\n",
    "                partition_key = (\n",
    "                    file_info.get('series_id', ''),\n",
    "                    file_info.get('year', ''),\n",
    "                    file_info.get('month', ''),\n",
    "                    file_info.get('day', '')\n",
    "                )\n",
    "                if partition_key not in files_by_partition:\n",
    "                    files_by_partition[partition_key] = []\n",
    "                files_by_partition[partition_key].append(file_info)\n",
    "            \n",
    "            # Process each partition\n",
    "            for partition_key, partition_files in files_by_partition.items():\n",
    "                series_id, year, month, day = partition_key\n",
    "                print(f\"Processing partition: series_id={series_id}, year={year}, month={month}, day={day}\")\n",
    "                \n",
    "                file_paths = [f\"s3a://{S3_BUCKET}/{file_info['key']}\" for file_info in partition_files]\n",
    "                \n",
    "                try:\n",
    "                    # Read with more conservative schema inference\n",
    "                    df = spark.read \\\n",
    "                        .format(file_type) \\\n",
    "                        .option(\"inferSchema\", \"false\") \\\n",
    "                        .load(file_paths)\n",
    "                    \n",
    "                    # Apply robust schema inference with fallback\n",
    "                    df = infer_schema_with_fallback(df)\n",
    "                    \n",
    "                    # Add partition columns from file path information\n",
    "                    df = df.withColumn(\"series_id\", lit(series_id).cast(StringType()))\n",
    "                    df = df.withColumn(\"year\", lit(year).cast(LongType()))\n",
    "                    df = df.withColumn(\"month\", lit(month).cast(LongType()))\n",
    "                    df = df.withColumn(\"day\", lit(day).cast(LongType()))\n",
    "                    \n",
    "                    # Add unique identifier for deduplication\n",
    "                    df = df.withColumn(\"unique_id\", \n",
    "                        concat(\n",
    "                            col(\"series_id\"),\n",
    "                            lit(\"_\"),\n",
    "                            col(\"timestamp\")\n",
    "                        )\n",
    "                    )\n",
    "                    \n",
    "                    # Validate schema compatibility\n",
    "                    schema_compatible = validate_schema_compatibility(df, table_name)\n",
    "                    if not schema_compatible:\n",
    "                        print(f\"Schema incompatibility detected. Trying to harmonize schema...\")\n",
    "                        try:\n",
    "                            table_df = spark.table(table_name)\n",
    "                            for field in table_df.schema.fields:\n",
    "                                if field.name in df.columns:\n",
    "                                    df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error harmonizing schema: {str(e)}\")\n",
    "                    \n",
    "                    # Use merge to handle deduplication\n",
    "                    df.createOrReplaceTempView(\"source_df\")\n",
    "                    spark.sql(f\"\"\"\n",
    "                        MERGE INTO {table_name} AS target\n",
    "                        USING source_df AS source\n",
    "                        ON target.unique_id = source.unique_id\n",
    "                        WHEN MATCHED THEN UPDATE SET *\n",
    "                        WHEN NOT MATCHED THEN INSERT *\n",
    "                    \"\"\")\n",
    "                    \n",
    "                    record_count = df.count()\n",
    "                    for file_info in partition_files:\n",
    "                        metadata_records.append(FileMetadata(\n",
    "                            file_path=file_info['key'],\n",
    "                            file_type=file_type,\n",
    "                            directory=directory,\n",
    "                            last_modified=file_info['last_modified'],\n",
    "                            size=file_info['size'],\n",
    "                            table_name=table_name,\n",
    "                            record_count=record_count,\n",
    "                            series_id=file_info.get('series_id'),\n",
    "                            year=file_info.get('year'),\n",
    "                            month=file_info.get('month'),\n",
    "                            day=file_info.get('day')\n",
    "                        ))\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing partition: {str(e)}\")\n",
    "                    for file_info in partition_files:\n",
    "                        metadata_records.append(FileMetadata(\n",
    "                            file_path=file_info['key'],\n",
    "                            file_type=file_type,\n",
    "                            directory=directory,\n",
    "                            last_modified=file_info['last_modified'],\n",
    "                            size=file_info['size'],\n",
    "                            table_name=table_name,\n",
    "                            error_message=str(e),\n",
    "                            series_id=file_info.get('series_id'),\n",
    "                            year=file_info.get('year'),\n",
    "                            month=file_info.get('month'),\n",
    "                            day=file_info.get('day')\n",
    "                        ))\n",
    "        \n",
    "        else:  # asset_metadata\n",
    "            # Process all files at once for asset_metadata\n",
    "            file_paths = [f\"s3a://{S3_BUCKET}/{file_info['key']}\" for file_info in new_files]\n",
    "            \n",
    "            try:\n",
    "                # Read NDJSON files with specific options\n",
    "                df = spark.read \\\n",
    "                    .format(\"json\") \\\n",
    "                    .option(\"multiline\", \"false\") \\\n",
    "                    .option(\"lineSep\", \"\\n\") \\\n",
    "                    .option(\"inferSchema\", \"false\") \\\n",
    "                    .load(file_paths)\n",
    "                \n",
    "                # Apply robust schema inference with fallback\n",
    "                df = infer_schema_with_fallback(df)\n",
    "                \n",
    "                # Add unique identifier for deduplication\n",
    "                df = df.withColumn(\"unique_id\", col(\"asset_id\"))\n",
    "                \n",
    "                df.write \\\n",
    "                  .format(\"delta\") \\\n",
    "                  .mode(\"append\") \\\n",
    "                  .saveAsTable(table_name)\n",
    "                \n",
    "                record_count = df.count()\n",
    "                for file_info in new_files:\n",
    "                    metadata_records.append(FileMetadata(\n",
    "                        file_path=file_info['key'],\n",
    "                        file_type=file_type,\n",
    "                        directory=directory,\n",
    "                        last_modified=file_info['last_modified'],\n",
    "                        size=file_info['size'],\n",
    "                        table_name=table_name,\n",
    "                        record_count=record_count\n",
    "                    ))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing files: {str(e)}\")\n",
    "                for file_info in new_files:\n",
    "                    metadata_records.append(FileMetadata(\n",
    "                        file_path=file_info['key'],\n",
    "                        file_type=file_type,\n",
    "                        directory=directory,\n",
    "                        last_modified=file_info['last_modified'],\n",
    "                        size=file_info['size'],\n",
    "                        table_name=table_name,\n",
    "                        error_message=str(e)\n",
    "                    ))\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing directory {directory}: {str(e)}\")\n",
    "        for file_info in new_files:\n",
    "            metadata_records.append(FileMetadata(\n",
    "                file_path=file_info['key'],\n",
    "                file_type=file_type,\n",
    "                directory=directory,\n",
    "                last_modified=file_info['last_modified'],\n",
    "                size=file_info['size'],\n",
    "                table_name=\"\",\n",
    "                error_message=str(e)\n",
    "            ))\n",
    "    \n",
    "    return metadata_records\n",
    "\n",
    "def ensure_partition_column_types(df: DataFrame, directory: str) -> DataFrame:\n",
    "    \"\"\"Ensure partition columns have consistent data types\"\"\"\n",
    "    if directory in [\"agg\", \"raw\"]:\n",
    "        if \"year\" in df.columns:\n",
    "            df = df.withColumn(\"year\", col(\"year\").cast(LongType()))\n",
    "        if \"month\" in df.columns:\n",
    "            df = df.withColumn(\"month\", col(\"month\").cast(LongType()))\n",
    "        if \"day\" in df.columns:\n",
    "            df = df.withColumn(\"day\", col(\"day\").cast(LongType()))\n",
    "        if \"series_bucket\" in df.columns:\n",
    "            df = df.withColumn(\"series_bucket\", col(\"series_bucket\").cast(StringType()))\n",
    "    elif directory == \"index\":\n",
    "        if \"series_id\" in df.columns:\n",
    "            df = df.withColumn(\"series_id\", col(\"series_id\").cast(StringType()))\n",
    "        if \"year\" in df.columns:\n",
    "            df = df.withColumn(\"year\", col(\"year\").cast(LongType()))\n",
    "        if \"month\" in df.columns:\n",
    "            df = df.withColumn(\"month\", col(\"month\").cast(LongType()))\n",
    "        if \"day\" in df.columns:\n",
    "            df = df.withColumn(\"day\", col(\"day\").cast(LongType()))\n",
    "    return df\n",
    "\n",
    "def validate_schema_compatibility(df: DataFrame, table_name: str) -> bool:\n",
    "    \"\"\"Validate if DataFrame schema is compatible with existing table\"\"\"\n",
    "    try:\n",
    "        # Get the table schema\n",
    "        table_df = spark.table(table_name)\n",
    "        table_schema = table_df.schema\n",
    "        df_schema = df.schema\n",
    "        \n",
    "        # Check for columns that exist in both with different types\n",
    "        for table_field in table_schema.fields:\n",
    "            for df_field in df_schema.fields:\n",
    "                if table_field.name == df_field.name and table_field.dataType != df_field.dataType:\n",
    "                    print(f\"Schema conflict: Column '{table_field.name}' has different types - \"\n",
    "                          f\"Table: {table_field.dataType}, DataFrame: {df_field.dataType}\")\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating schema: {str(e)}\")\n",
    "        return True  # Assume compatible if we can't check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da94d48-f99a-4eca-aa23-9207b0394291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Section 6: Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31df50c3-db31-4dc5-9db7-2d3b48b43bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        # Initialize S3 configuration from environment variables\n",
    "        s3_config = S3Config.from_env()\n",
    "        \n",
    "        # Create S3 client\n",
    "        s3_client = get_s3_client(s3_config)\n",
    "        \n",
    "        # Create metadata table if it doesn't exist\n",
    "        create_metadata_table()\n",
    "        \n",
    "        # List files in each directory\n",
    "        files_by_directory = list_s3_files(s3_client, s3_config)\n",
    "        \n",
    "        # Process each directory\n",
    "        all_metadata_records = []\n",
    "        for directory, files in files_by_directory.items():\n",
    "            print(f\"Processing directory: {directory}\")\n",
    "            metadata_records = process_directory_files(directory, files)\n",
    "            all_metadata_records.extend(metadata_records)\n",
    "        \n",
    "        # Update metadata\n",
    "        if all_metadata_records:\n",
    "            update_metadata(all_metadata_records)\n",
    "            print(f\"Processed {len(all_metadata_records)} new files\")\n",
    "        else:\n",
    "            print(\"No new files to process\")\n",
    "        \n",
    "        print(\"ETL process completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in ETL process: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Run the main process\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "s3_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
