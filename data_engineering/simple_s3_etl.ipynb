{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ea1f7c-e209-4e3f-bc62-298eed895870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from delta.tables import DeltaTable\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Dict, Any\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import boto3\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import col, lit, current_timestamp, input_file_name, regexp_extract, concat\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType, ArrayType, MapType, DoubleType\n",
    "\n",
    "# S3 Path to Read From (Modify as Needed)\n",
    "\n",
    "# Configuration constants\n",
    "CATALOG_NAME = \"tulip_sandbox\"\n",
    "SCHEMA_NAME = \"sitewise_test\"\n",
    "S3_BUCKET = os.getenv('BUCKET_NAME', 'hannover-messe-tulip')\n",
    "BASE_PREFIX = os.getenv('BASE_PREFIX', 'iot-sitewise/')\n",
    "METADATA_TABLE = f\"{CATALOG_NAME}.{SCHEMA_NAME}.file_metadata\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\")\n",
    "\n",
    "def get_aws_credentials():\n",
    "    \"\"\"Get AWS credentials from environment variables\"\"\"\n",
    "    access_key = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "    secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "    \n",
    "    if not access_key or not secret_key:\n",
    "        raise ValueError(\"AWS credentials not found in environment variables\")\n",
    "    \n",
    "    return access_key, secret_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c319a924-f4bd-4ffa-a794-e3abb3071e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class S3Config:\n",
    "    \"\"\"Configuration for S3 access\"\"\"\n",
    "    bucket: str\n",
    "    aws_access_key_id: str\n",
    "    aws_secret_access_key: str\n",
    "    region: str = os.getenv('AWS_REGION', 'us-east-1')\n",
    "    base_prefix: str = BASE_PREFIX\n",
    "\n",
    "    @classmethod\n",
    "    def from_env(cls) -> 'S3Config':\n",
    "        \"\"\"Create S3Config from environment variables\"\"\"\n",
    "        access_key, secret_key = get_aws_credentials()\n",
    "        return cls(\n",
    "            bucket=S3_BUCKET,\n",
    "            aws_access_key_id=access_key,\n",
    "            aws_secret_access_key=secret_key\n",
    "        )\n",
    "\n",
    "\n",
    "def get_s3_client(config: S3Config) -> boto3.client:\n",
    "    \"\"\"Create S3 client with explicit credentials\"\"\"\n",
    "    return boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=config.aws_access_key_id,\n",
    "        aws_secret_access_key=config.aws_secret_access_key,\n",
    "        region_name=config.region\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a8855e5-c928-40e6-81b3-5954211754ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = (spark.read.format(\"avro\")\n",
    "      .load(wildcard_uri)\n",
    "      .withColumn(\"file_name\", input_file_name()))  # Add file source for tracking\n",
    "\n",
    "\n",
    "\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52bce0ef-11e2-4fe3-80dc-a405a36350cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Check if Unity Catalog table exists\n",
    "if spark._jsparkSession.catalog().tableExists(catalog_table):\n",
    "    delta_table = DeltaTable.forName(spark, catalog_table)\n",
    "\n",
    "    # Define composite key for merge\n",
    "    merge_condition = \"\"\"\n",
    "        tgt.seriesId = src.seriesId AND\n",
    "        tgt.timeInSeconds = src.timeInSeconds AND\n",
    "        tgt.file_name = src.file_name\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform merge (upsert)\n",
    "    delta_table.alias(\"tgt\").merge(\n",
    "        df.alias(\"src\"),\n",
    "        merge_condition\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "else:\n",
    "    # If the table doesn't exist, create it\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(catalog_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a8ab621-fc61-443d-ad3b-2a40c7edba01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "âœ… Use this format:\n",
    "\n",
    "pgsql\n",
    "Copy\n",
    "Edit\n",
    "s3://bucket-name/path/to/files/\n",
    "For your case, it should be:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "s3_path = \"s3://bucket_name/sitewise/raw/startYear=*/startMonth=*/startDay=*/*.avro\"\n",
    "PySpark's spark.read.format(\"avro\").load(s3_path) supports this."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "simple_s3_etl",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
